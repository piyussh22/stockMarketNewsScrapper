{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12322162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkGenerator(word):\n",
    "    link='https://news.google.com/rss/search?q='\n",
    "    link+=word\n",
    "    return link\n",
    "#     print(link)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bce362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.1-cp39-cp39-win_amd64.whl (10.7 MB)\n",
      "Collecting numpy>=1.20.3\n",
      "  Downloading numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\padamshree\\desktop\\project 6th sem\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\padamshree\\desktop\\project 6th sem\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, numpy, pandas\n",
      "Successfully installed numpy-1.24.3 pandas-2.0.1 pytz-2023.3 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\padamshree\\desktop\\project 6th sem\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec5e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def xmlparse(link):\n",
    "    tree = ET.parse(link)\n",
    "# print(type(tree))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    title=[]\n",
    "    link=[] \n",
    "    desc=[]\n",
    "    source=[]\n",
    "    date=[]\n",
    "    for child in root:\n",
    "        for i in range(8,len(child)):\n",
    "            for j in range(len(child[i])):\n",
    "                if child[i][j].tag=='title':\n",
    "                    title.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='link':\n",
    "                    link.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='description':\n",
    "                    desc.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='source':\n",
    "                    source.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='pubDate':\n",
    "                    date.append(child[i][j].text)\n",
    "    dict={'title':title,'link':link,'pubdate':date,'source':source}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01e2f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def businessToday(link):\n",
    "    response=requests.get(link)\n",
    "    author=''\n",
    "    soup=BeautifulSoup(response.text,'html.parser')   \n",
    "    content=soup.find('div',{'class':'content-area'})\n",
    "    try:\n",
    "        heading=content.find('div',{'class':'story-heading'}).text\n",
    "    except:\n",
    "        heading='No heading'\n",
    "    #     print(heading)\n",
    "    try:\n",
    "        subheading=content.find('div',{'class':'sab-head-tranlate-sec'}).text\n",
    "    except:\n",
    "        subheading='No subheading'\n",
    "        #     print(subheading)\n",
    "    try:\n",
    "        userdetail=content.find('div',{'class':'userdetail-share-main story-user-section'})\n",
    "        left=userdetail.find('div',{'class':'user-detial-left'})\n",
    "        branddetail=left.find('div',{'class':'brand-detial-main'})\n",
    "        date=branddetail.find('ul').text\n",
    "        date=date.replace('Updated ','')\n",
    "    except:\n",
    "        date='No date'\n",
    "        #     print(date)\n",
    "    try:\n",
    "        story=content.find('div',{'class':'stroy-870'})\n",
    "        main=story.find('div',{'class':'story-with-main-sec'})\n",
    "        storyFormat=main.find('div',{'class':'text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item'})\n",
    "        p=storyFormat.find_all('p')\n",
    "        news=[]\n",
    "        for i in p:\n",
    "            news.append(i.text)\n",
    "    except:\n",
    "        news='No data'\n",
    "            #     print(data)\n",
    "    dict={'Source':['Business Today'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[date],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "871e4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic class\n",
    "def economicTimes(link):\n",
    "    author=''\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'article_wrap'})\n",
    "    headline=content.find('div',{'class':'topPart clearfix tac fixedOnLoad'})\n",
    "    heading=headline.find('h1').text\n",
    "    subheading=''\n",
    "#     print(heading)\n",
    "    metadata=content.find('div',{'class':'bylineBox'})\n",
    "    contentwrapper=metadata.find('div',{'class':'dt contentwrapper'})\n",
    "    artbyline=contentwrapper.find('div',{'class':'dtc vam artByline'})\n",
    "    time=artbyline.find('time',{'class':'jsdtTime'}).text\n",
    "    time=time.replace('Last Updated: ','')\n",
    "#     print(time)\n",
    "#     author=artbyline.find('div',{'class':'auth eventDone'})\n",
    "#     a_nam=author.find('a').text\n",
    "#     print(a_name)\n",
    "    news=[]\n",
    "    newsdata=content.find('div',{'class':'edition clearfix'})\n",
    "    pagecontent=newsdata.find('div',{'class':'pageContent flt'})\n",
    "    article=pagecontent.find('article',{'class':'artData clr paywall'}).text\n",
    "#     p=article.find('p').text\n",
    "#     print(article)\n",
    "#     arttext=article.find('div',{'class':'artText medium'}).text\n",
    "#     br=arttext.find('em')\n",
    "#     for i in br:\n",
    "#         news.append(i.text)\n",
    "#     print(br)\n",
    "    dict={'Source':['Economic Times'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[time],'news':[article]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27629ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mint(link):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('section',{'class':'mainSec'})\n",
    "    try:\n",
    "        head=content.find('div',{'class':'stickyCare'})\n",
    "        heading=head.find('h1',{'class':'headline'}).text\n",
    "    except:\n",
    "        heading='no heading'\n",
    "    data=[]\n",
    "    subheading=''\n",
    "#     print(heading)\n",
    "    try:\n",
    "        publish=head.find('span',{'class':'articleInfo pubtime'})\n",
    "        lastUpdate=publish.find_all('span')[1].text\n",
    "        lastUpdate=lastUpdate.replace('Updated: ','')\n",
    "    except:\n",
    "        lastUpdate='no date'\n",
    "        #     print(lastUpdate)\n",
    "    try:\n",
    "        author=head.find('span',{'class':'articleInfo author'})\n",
    "        name=author.find_all('a')\n",
    "        for i in name:\n",
    "            a_name=i.find('strong').text\n",
    "#             a_name=i.text\n",
    "    except:\n",
    "        a_name='No author'\n",
    "\n",
    "#     print(a_name)\n",
    "    try:\n",
    "        news=content.find('div',{'class':'contentSec'})\n",
    "        first=news.find('div',{'class':'FirstEle'})\n",
    "        data.append(first.text)\n",
    "    #     print(first.text)\n",
    "        second=news.find('div',{'class':'paywall'})\n",
    "        p=second.find_all('p');\n",
    "        for i in p:\n",
    "            data.append(i.text)\n",
    "    except:\n",
    "        data='No data'\n",
    "#         print(i.text)\n",
    "    dict={'Source':['Mint'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[lastUpdate],'news':[data]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18b81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def businessStandard(link):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'story-detail'})\n",
    "    author=''\n",
    "    try:\n",
    "        heading=content.find('h1').text\n",
    "    except:\n",
    "        heading='no heading'\n",
    "#     print(heading.text)\n",
    "    try:\n",
    "        subheading=content.find('h2').text\n",
    "    except:\n",
    "        subheading='no sub'\n",
    "#     print(subheading.text)\n",
    "    try:\n",
    "        data=content.find('div',{'class':'storycontent'})\n",
    "        p=data.find_all('p')\n",
    "        if p==[]:\n",
    "            p=data.find_all('div')\n",
    "        news=[]\n",
    "        for i in p:\n",
    "            news.append(i.text)\n",
    "    except:\n",
    "        news='no news'\n",
    "            #     print(news)\n",
    "    try:\n",
    "        date=content.find('div',{'class':'story-first-time'})\n",
    "        p=date.find('p').text\n",
    "        p=p.replace('First Published: ','')\n",
    "    except:\n",
    "        p='no date'\n",
    "        #     print(p)\n",
    "    dict={'Source':['Business Standard'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[p],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50729f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moneyControl(link):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'page_left_wrapper'})\n",
    "    try:\n",
    "        heading=content.find('h1',{'class':'article_title artTitle'}).text\n",
    "    except:\n",
    "        heading=''\n",
    "        #     print(heading)\n",
    "    try:\n",
    "        subheading=content.find('h2',{'class':'article_desc'}).text\n",
    "    except:\n",
    "        subheading=''\n",
    "        #     print(subheading)\n",
    "    try:   \n",
    "        metadata=content.find('div',{'class':'clearfix'})\n",
    "        author=metadata.find('div',{'class':'article_author'}).text\n",
    "        author=author\n",
    "    except:\n",
    "        author=''\n",
    "        #     print(author.strip())\n",
    "    try:\n",
    "        dates=metadata.find('div',{'class':'article_schedule'}).text\n",
    "        dates=dates\n",
    "    except:\n",
    "        dates=''\n",
    "#     print(dates.strip())\n",
    "    try:\n",
    "        data=content.find('div',{'class':'content_wrapper arti-flow'})\n",
    "        p=data.find_all('p')\n",
    "        news=[]\n",
    "        for i in p:\n",
    "            news.append(i.text)\n",
    "    except:\n",
    "        news=''\n",
    "#     print(news)\n",
    "    dict={'Source':['Money Control'],'heading':[heading],'subheading':[subheading],'author':[author.strip()],'date':[dates.strip()],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb60f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toi(link):\n",
    "    response=requests.get(link)\n",
    "    author=''\n",
    "    subheading=''\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'okf2Z'})\n",
    "    try:\n",
    "#         print('hejjs')\n",
    "        heading=content.find('div',{'class':'pZFl7'}).text\n",
    "    except:\n",
    "        heading=''\n",
    "        #     print(heading.text)\n",
    "    try:\n",
    "        time=heading.find('div',{'class':'t8vf3 byline_action'})\n",
    "        date=time.find('span').text\n",
    "    except:\n",
    "        date=''\n",
    "#     print(date)\n",
    "    data=content.find('div',{'class':'JuyWl'})\n",
    "    data1=data.find('div',{'class':'vSlIC'})\n",
    "    data2=data1.find('div',{'class':'heightCalc'})\n",
    "    x=soup.find_all('div')\n",
    "    for i in x:\n",
    "        y=i.get('class')\n",
    "        if 'fewcent' in str(y):\n",
    "            classs=y\n",
    "    data3=data2.find('div',{'class':classs})\n",
    "    news=data3.find('div',{'class':'_s30J clearfix'}).text\n",
    "#     news=''\n",
    "        #     print(news)\n",
    "    dict={'Source':['TOI'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[date],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4416dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmlSaver(link):\n",
    "    import requests\n",
    "\n",
    "#     URL = \"https://news.google.com/rss/search?q=NIFTY\"\n",
    "    URL=link\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    with open('file.xml', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "920777b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Using legacy 'setup.py install' for bs4, since package 'wheel' is not installed.\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "    Running setup.py install for bs4: started\n",
      "    Running setup.py install for bs4: finished with status 'done'\n",
      "Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\padamshree\\desktop\\project 6th sem\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cceee6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requestsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.1.0 idna-3.4 requests-2.29.0 urllib3-1.26.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\padamshree\\desktop\\project 6th sem\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508fdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrappers(df,index):\n",
    "    source=df['source'][index]\n",
    "    if source=='Mint':\n",
    "        dataframe=mint(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Business Standard':\n",
    "        dataframe=businessStandard(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Economic Times':\n",
    "        dataframe=economicTimes(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Business Today':\n",
    "        dataframe=businessToday(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Times of India':\n",
    "        dataframe=toi(df['link'][index])\n",
    "    elif source=='Moneycontrol':\n",
    "        dataframe=moneyControl(df['link'][index])    \n",
    "#     else:\n",
    "#         print('working on it!!!')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "245febc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    word=input(\"Enter stock symbol to scrap\\n\")\n",
    "    link=linkGenerator(word)\n",
    "    file=xmlSaver(link)\n",
    "#     print(file)\n",
    "#     df=xmlparse('C:\\\\Users\\\\PadamShree\\\\Desktop\\\\project 6th sem\\\\file.xml')\n",
    "    from pathlib import Path\n",
    "    link=Path.home()\n",
    "    link=str(link)+'\\\\Downloads\\\\file.xml'\n",
    "    df=xmlparse(link)\n",
    "#     df=link\n",
    "    counts=df['source'].value_counts()\n",
    "\n",
    "\n",
    "    temp_dict={}\n",
    "    df_all=pd.DataFrame(temp_dict)\n",
    "    for i in range(0,100):\n",
    "        if df['source'][i]=='Economic Times' :\n",
    "            continue\n",
    "#         df_all.append(scrappers(df,i))\n",
    "#     return df_all\n",
    "        \n",
    "        dict=scrappers(df,i)\n",
    "#         print(dict)\n",
    "        df_all=pd.concat([dict,df_all])\n",
    "    return df_all\n",
    "#     scrappers(df,0)\n",
    "#     scrappers(df,1)\n",
    "#     scrappers(df,2)\n",
    "#     scrappers(df,3)\n",
    "#     df=scrappers(df,4)\n",
    "#     print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce16dd83",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     df = df.transpose()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     word\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter stock symbol to scrap\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     link\u001b[38;5;241m=\u001b[39mlinkGenerator(word)\n\u001b[0;32m      4\u001b[0m     file\u001b[38;5;241m=\u001b[39mxmlSaver(link)\n",
      "File \u001b[1;32mc:\\users\\padamshree\\desktop\\project 6th sem\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\padamshree\\desktop\\project 6th sem\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df = main()\n",
    "#     df = df.transpose()\n",
    "    df.to_csv('main.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dea1c5b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec776f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
